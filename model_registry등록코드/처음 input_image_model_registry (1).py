#!/usr/bin/env python
# coding: utf-8

# !pip install protobuf==3.20.*

# In[1]:


import tensorflow as tf
import autokeras as ak

print("âœ… TensorFlow:", tf.__version__)
print("âœ… AutoKeras:", ak.__version__)
print("âœ… GPU:", tf.config.list_physical_devices('GPU'))


# In[2]:


# ìƒ˜í”Œ ë°ì´í„°ë¡œ 
import os
import io
import boto3
import psycopg2
import numpy as np
import pandas as pd
from PIL import Image
from datetime import datetime
from tqdm import tqdm
import tensorflow as tf
import autokeras as ak
import mlflow
import mlflow.keras
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, roc_auc_score

# ------------------------- ì„¤ì • -------------------------
image_size = (300, 300)
sample_size = 3000

AWS_ACCESS_KEY = 'AKIA2UMF2NRTTGTVHHUD'
AWS_SECRET_KEY = 'Z/90YNZ1tiH2e8QEeXObw+incc99QIDWsrfN2bTb'
BUCKET_NAME = 'smart-factory-datalake'

DB_INFO = {
    'host': 'localhost',
    'port': 5432,
    'dbname': 'postgres',
    'user': 'postgres',
    'password': '0523'
}

s3 = boto3.client("s3", aws_access_key_id=AWS_ACCESS_KEY, aws_secret_access_key=AWS_SECRET_KEY)

def fetch_sampled_images(upload_id, sample_size):
    print(f"\nğŸŸ¢ DBì—ì„œ upload_id={upload_id} ë°ì´í„° ë¡œë”© ì¤‘")
    conn = psycopg2.connect(**DB_INFO)
    df = pd.read_sql("""
        SELECT id AS image_id, image_path, is_defect
        FROM input_image
        WHERE upload_id = %s
    """, conn, params=(upload_id,))
    present_classes = set(df["is_defect"].unique())
    if len(present_classes) < 2:
        missing_class = 1 if 0 in present_classes else 0
        print(f"âš ï¸ í´ë˜ìŠ¤ {missing_class} ë³´ì™„ì„ ìœ„í•´ DBì—ì„œ ì¶”ê°€ ì¡°íšŒ")
        df_add = pd.read_sql(f"""
            SELECT id AS image_id, image_path, is_defect
            FROM input_image
            WHERE is_defect = {missing_class} AND upload_id != %s
            ORDER BY RANDOM()
            LIMIT 10
        """, conn, params=(upload_id,))
        df = pd.concat([df, df_add], ignore_index=True)
    conn.close()
    if df.empty:
        raise ValueError(f"âŒ upload_id {upload_id}ì— í•´ë‹¹í•˜ëŠ” ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
    df_sampled = df.sample(n=min(sample_size, len(df)), random_state=42).reset_index(drop=True)
    print(f"âœ… ìƒ˜í”Œë§ ì™„ë£Œ: {len(df_sampled)}ê°œ (ë‘ í´ë˜ìŠ¤ í¬í•¨ë¨)")
    x, y = [], []
    for _, row in tqdm(df_sampled.iterrows(), total=len(df_sampled), desc="ğŸ“¦ ì´ë¯¸ì§€ ë¡œë”©"):
        try:
            key = row["image_path"]
            label = int(row["is_defect"])
            obj = s3.get_object(Bucket=BUCKET_NAME, Key=key)
            img = Image.open(io.BytesIO(obj["Body"].read())).convert("RGB").resize(image_size)
            x.append(np.array(img) / 255.0)
            y.append(label)
        except Exception as e:
            print(f"âš ï¸ {key} â†’ {e}")
    return np.array(x, dtype=np.float32), np.array(y, dtype=np.int32)

def train_and_log_model(upload_id):
    x, y = fetch_sampled_images(upload_id, sample_size)
    if len(np.unique(y)) < 2:
        raise ValueError("âŒ í´ë˜ìŠ¤ê°€ í•˜ë‚˜ë§Œ ì¡´ì¬í•©ë‹ˆë‹¤. ìµœì†Œ ë‘ í´ë˜ìŠ¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, stratify=y, random_state=42)
    clf = ak.ImageClassifier(overwrite=True, max_trials=1)
    clf.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))
    from sklearn.metrics import accuracy_score, f1_score, roc_auc_score
    try:
        y_pred_probs = clf.predict_proba(x_test)
    except AttributeError:
        y_pred_probs = clf.predict(x_test)
        print("âš ï¸ predict_probaê°€ ì—†ìŒ. predict ì‚¬ìš©. í™•ë¥  ì¶œë ¥ìœ¼ë¡œ ë³€í™˜:")
        y_pred_probs = y_pred_probs.flatten()  # (n_samples, 1) â†’ (n_samples,)
    y_pred = np.round(y_pred_probs).astype(int).flatten()
    try:
        accuracy = accuracy_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred, average='binary', zero_division=0)
        n_classes = len(np.unique(y_test))
        if n_classes < 2:
            auc = 0.0
        else:
            auc = roc_auc_score(y_test, y_pred_probs)
        print(f"Accuracy: {accuracy:.4f}")
        print(f"F1-Score: {f1:.4f}")
        print(f"AUC: {auc:.4f}")
    except Exception as e:
        print(f"âš ï¸ ì§€í‘œ ê³„ì‚° ì‹¤íŒ¨: {e}")
        print(f"ë””ë²„ê¹… ì •ë³´ - y_test: {np.unique(y_test, return_counts=True)}")
        print(f"ë””ë²„ê¹… ì •ë³´ - y_pred_probs shape: {y_pred_probs.shape}")
        accuracy, f1, auc = 0.0, 0.0, 0.0
    model = clf.export_model()
    mlflow.set_tracking_uri("http://localhost:5000")
    mlflow.set_experiment("SmartFactory_Image_Models")
    with mlflow.start_run() as run:
        mlflow.keras.log_model(model, "model")
        mlflow.log_metrics({"accuracy": accuracy, "f1_score": f1, "auc_score": auc})
        mlflow_run_id = run.info.run_id
        model_name = run.info.run_name
        version = datetime.now().strftime("v%Y%m%d_%H%M%S")
        conn = psycopg2.connect(**DB_INFO)
        cur = conn.cursor()
        cur.execute('''
            INSERT INTO model_registry (
                model_name, version, mlflow_run_id,
                trained_on_upload_id, data_type,
                accuracy, f1_score, auc_score,
                is_active, created_at
            ) VALUES (%s, %s, %s, %s, 'image', %s, %s, %s, TRUE, NOW())
        ''', (model_name, version, mlflow_run_id, upload_id, accuracy, f1, auc))
        conn.commit()
        cur.close()
        conn.close()
        print(f"âœ… ëª¨ë¸ ë“±ë¡ ì™„ë£Œ: {model_name}, version={version}, Run ID={mlflow_run_id}")

if __name__ == "__main__":
    upload_id = 1
    train_and_log_model(upload_id)


# In[ ]:


import os
os.environ["CUDA_VISIBLE_DEVICES"] = "-1"  # GPU ì‚¬ìš© ì•ˆ í•¨
# ì „ì²´ ë°ì´í„°ë¡œ
import os
import io
import boto3
import psycopg2
import numpy as np
import pandas as pd
from PIL import Image
from datetime import datetime
from tqdm import tqdm
import tensorflow as tf
import autokeras as ak
import mlflow
import mlflow.keras
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, roc_auc_score
# ------------------------- ì„¤ì • -------------------------

# TensorFlow GPU ë¹„í™œì„±í™” ëª…ì‹œ
physical_devices = tf.config.list_physical_devices('GPU')
if physical_devices:
    tf.config.set_visible_devices([], 'GPU')
    print("âœ… GPU ë¹„í™œì„±í™” ì™„ë£Œ")
else:
    print("âœ… GPU ì—†ìŒ, CPUë¡œ ì§„í–‰")

DB_INFO = {
    'host': 'localhost',
    'port': 5432,
    'dbname': 'postgres',
    'user': 'postgres',
    'password': '0523'
}

AWS_ACCESS_KEY = 'AKIA2UMF2NRTTGTVHHUD'
AWS_SECRET_KEY = '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'
BUCKET_NAME = 'smart-factory-datalake'

image_size = (300, 300)

s3 = boto3.client("s3", aws_access_key_id=AWS_ACCESS_KEY, aws_secret_access_key=AWS_SECRET_KEY)

def fetch_sampled_images(upload_id):
    print(f"\nğŸŸ¢ DBì—ì„œ upload_id={upload_id} ë°ì´í„° ë¡œë”© ì¤‘")
    conn = psycopg2.connect(**DB_INFO)
    df = pd.read_sql("""
        SELECT id AS image_id, image_path, is_defect
        FROM input_image
        WHERE upload_id = %s
    """, conn, params=(upload_id,))
    present_classes = set(df["is_defect"].unique())
    if len(present_classes) < 2:
        missing_class = 1 if 0 in present_classes else 0
        print(f"âš ï¸ í´ë˜ìŠ¤ {missing_class} ë³´ì™„ì„ ìœ„í•´ DBì—ì„œ ì¶”ê°€ ì¡°íšŒ")
        df_add = pd.read_sql(f"""
            SELECT id AS image_id, image_path, is_defect
            FROM input_image
            WHERE is_defect = {missing_class} AND upload_id != %s
            ORDER BY RANDOM()
            LIMIT 10
        """, conn, params=(upload_id,))
        df = pd.concat([df, df_add], ignore_index=True)
    conn.close()
    if df.empty:
        raise ValueError(f"âŒ upload_id {upload_id}ì— í•´ë‹¹í•˜ëŠ” ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
    df_full = df.reset_index(drop=True)  # ìƒ˜í”Œë§ ì œê±°, ì „ì²´ ë°ì´í„° ì‚¬ìš©
    print(f"âœ… ì „ì²´ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df_full)}ê°œ (ë‘ í´ë˜ìŠ¤ í¬í•¨ë¨)")
    x, y = [], []
    for _, row in tqdm(df_full.iterrows(), total=len(df_full), desc="ğŸ“¦ ì´ë¯¸ì§€ ë¡œë”©"):
        try:
            key = row["image_path"]
            label = int(row["is_defect"])
            obj = s3.get_object(Bucket=BUCKET_NAME, Key=key)
            img = Image.open(io.BytesIO(obj["Body"].read())).convert("RGB").resize(image_size)
            x.append(np.array(img) / 255.0)
            y.append(label)
        except Exception as e:
            print(f"âš ï¸ {key} â†’ {e}")
    return np.array(x, dtype=np.float32), np.array(y, dtype=np.int32)

def train_and_log_model(upload_id):
    x, y = fetch_sampled_images(upload_id)
    if len(np.unique(y)) < 2:
        raise ValueError("âŒ í´ë˜ìŠ¤ê°€ í•˜ë‚˜ë§Œ ì¡´ì¬í•©ë‹ˆë‹¤. ìµœì†Œ ë‘ í´ë˜ìŠ¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    
    # ë°ì´í„° í¬ê¸° í™•ì¸
    print(f"ğŸ“ ë°ì´í„° í¬ê¸° - x: {x.shape}, y: {y.shape}")
    
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, stratify=y, random_state=42)
    
    # í•™ìŠµ ë°ì´í„° í¬ê¸° í™•ì¸
    print(f"ğŸ“ í•™ìŠµ ë°ì´í„° í¬ê¸° - x_train: {x_train.shape}, y_train: {y_train.shape}")
    
    clf = ak.ImageClassifier(overwrite=True, max_trials=1)
    try:
        clf.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))
    except Exception as e:
        print(f"âŒ í•™ìŠµ ì‹¤íŒ¨: {e}")
        print(f"ë””ë²„ê¹… ì •ë³´ - x_train shape: {x_train.shape}, y_train shape: {y_train.shape}")
        raise
    
    from sklearn.metrics import accuracy_score, f1_score, roc_auc_score
    try:
        y_pred_probs = clf.predict_proba(x_test)
    except AttributeError:
        y_pred_probs = clf.predict(x_test)
        print("âš ï¸ predict_probaê°€ ì—†ìŒ. predict ì‚¬ìš©. í™•ë¥  ì¶œë ¥ìœ¼ë¡œ ë³€í™˜:")
        y_pred_probs = y_pred_probs.flatten()  # (n_samples, 1) â†’ (n_samples,)
    y_pred = np.round(y_pred_probs).astype(int).flatten()
    try:
        accuracy = accuracy_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred, average='binary', zero_division=0)
        n_classes = len(np.unique(y_test))
        if n_classes < 2:
            auc = 0.0
        else:
            auc = roc_auc_score(y_test, y_pred_probs)
        print(f"Accuracy: {accuracy:.4f}")
        print(f"F1-Score: {f1:.4f}")
        print(f"AUC: {auc:.4f}")
    except Exception as e:
        print(f"âš ï¸ ì§€í‘œ ê³„ì‚° ì‹¤íŒ¨: {e}")
        print(f"ë””ë²„ê¹… ì •ë³´ - y_test: {np.unique(y_test, return_counts=True)}")
        print(f"ë””ë²„ê¹… ì •ë³´ - y_pred_probs shape: {y_pred_probs.shape}")
        accuracy, f1, auc = 0.0, 0.0, 0.0
    model = clf.export_model()
    mlflow.set_tracking_uri("http://localhost:5000")
    mlflow.set_experiment("SmartFactory_Image_Models")
    with mlflow.start_run() as run:
        mlflow.keras.log_model(model, "model")
        mlflow.log_metrics({"accuracy": accuracy, "f1_score": f1, "auc_score": auc})
        mlflow_run_id = run.info.run_id
        model_name = run.info.run_name
        version = datetime.now().strftime("v%Y%m%d_%H%M%S")
        conn = psycopg2.connect(**DB_INFO)
        cur = conn.cursor()
        cur.execute('''
            INSERT INTO model_registry (
                model_name, version, mlflow_run_id,
                trained_on_upload_id, data_type,
                accuracy, f1_score, auc_score,
                is_active, created_at
            ) VALUES (%s, %s, %s, %s, 'image', %s, %s, %s, TRUE, NOW())
        ''', (model_name, version, mlflow_run_id, upload_id, accuracy, f1, auc))
        conn.commit()
        cur.close()
        conn.close()
        print(f"âœ… ëª¨ë¸ ë“±ë¡ ì™„ë£Œ: {model_name}, version={version}, Run ID={mlflow_run_id}")

if __name__ == "__main__":
    upload_id = 1
    train_and_log_model(upload_id)


# In[ ]:


# 1. Test dataë¥¼ DB ì ì¬ í›„ 
# 2. ê¸°ì¡´ ë°ì´í„° ì ì¬ ìˆ˜(input_imageì˜ ìˆ˜ count)ì™€ ìƒˆë¡œ ë“¤ì–´ì˜¨ Test data ìˆ˜ ë¹„êµ 
# 


# In[ ]:


# 3. ëª¨ë¸ ì¬í•™ìŠµ(autokeras)


# In[ ]:


# 4. ê¸°ì¡´ ëª¨ë¸ vs ìƒˆ ëª¨ë¸


# In[ ]:


# 5. ì‚¬ìš©ì ì„ íƒ


# In[ ]:


# 6. ìƒˆ ëª¨ë¸ registry ë“±ë¡


# In[ ]:


# 7. ìš´ì˜ ë°°í¬ íŒŒì´í”„ë¼ì¸ ì—°ë™


# In[3]:


# test ìœ„í•œ S3 test ë°ì´í„° ì¶”ì¶œ í›„ DB ì ì¬ 
import os
import boto3
import psycopg2
import random
from datetime import datetime
from PIL import Image
import io

# í™˜ê²½ ì„¤ì •
AWS_ACCESS_KEY = 'AKIA2UMF2NRTTGTVHHUD'
AWS_SECRET_KEY = 'Z/90YNZ1tiH2e8QEeXObw+incc99QIDWsrfN2bTb'
BUCKET_NAME = 'smart-factory-datalake'
PREFIX = 'test/'  # âœ… test ë°ì´í„° ìœ„ì¹˜
SAMPLE_COUNT = 10          # âœ… ê³ ì •ëœ ê°œìˆ˜ ìƒ˜í”Œë§

# í…ŒìŠ¤íŠ¸ìš© ê³ ì • ID
COMPANY_ID = '11111111-1111-1111-1111-111111111111'
UPLOADER_ID = '22222222-2222-2222-2222-222222222222'

DB_INFO = {
    'host': 'localhost',
    'port': 5432,
    'dbname': 'postgres',
    'user': 'postgres',
    'password': '0523'
}

s3 = boto3.client("s3", aws_access_key_id=AWS_ACCESS_KEY, aws_secret_access_key=AWS_SECRET_KEY)

# Step 1. S3 í‚¤ ë¦¬ìŠ¤íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸°
def list_image_keys(bucket, prefix):
    response = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)
    return [obj['Key'] for obj in response.get('Contents', []) if obj['Key'].lower().endswith(('.jpg', '.jpeg', '.png'))]

# Step 2. 10ê°œë§Œ ìƒ˜í”Œë§
def sample_image_keys(all_keys, count):
    return random.sample(all_keys, min(count, len(all_keys)))

# Step 3. uploads í…Œì´ë¸”ì— ì—…ë¡œë“œ ë©”íƒ€ ìƒì„±
def create_upload_entry(file_key_prefix):
    conn = psycopg2.connect(**DB_INFO)
    cur = conn.cursor()
    cur.execute("""
        INSERT INTO uploads (company_id, uploader_id, file_key, data_type, has_target, label_type, task_type, created_at)
        VALUES (%s, %s, %s, 'image', FALSE, 'binary', NULL, NOW())
        RETURNING upload_id
    """, (COMPANY_ID, UPLOADER_ID, file_key_prefix))
    upload_id = cur.fetchone()[0]
    conn.commit()
    cur.close()
    conn.close()
    print(f"ğŸ“¥ uploads í…Œì´ë¸”ì— ë“±ë¡ ì™„ë£Œ â†’ upload_id: {upload_id}")
    return upload_id

# Step 4. S3 ì´ë¯¸ì§€ ë©”íƒ€ ì¶”ì¶œ
def get_image_metadata(key):
    obj = s3.get_object(Bucket=BUCKET_NAME, Key=key)
    img = Image.open(io.BytesIO(obj['Body'].read()))
    width, height = img.size
    img_format = img.format.lower()
    return width, height, img_format

# Step 5. input_imageì— ì‚½ì…
def insert_images_to_db(image_keys, upload_id):
    conn = psycopg2.connect(**DB_INFO)
    cur = conn.cursor()

    for key in image_keys:
        try:
            width, height, img_format = get_image_metadata(key)
            is_defect = random.choice([True, False])  # ì„ì˜ë¡œ ë¼ë²¨ ë¶€ì—¬
            cur.execute("""
                INSERT INTO input_image (upload_id, image_path, width, height, format, is_defect, created_at)
                VALUES (%s, %s, %s, %s, %s, %s, NOW())
            """, (upload_id, key, width, height, img_format, is_defect))
            print(f"âœ… ë“±ë¡ ì™„ë£Œ: {key}")
        except Exception as e:
            print(f"âš ï¸ ë“±ë¡ ì‹¤íŒ¨: {key} â†’ {e}")

    conn.commit()
    cur.close()
    conn.close()
    print(f"ğŸ“¦ ì´ {len(image_keys)}ê°œ ì´ë¯¸ì§€ ë“±ë¡ ì™„ë£Œ (upload_id: {upload_id})")

# ì „ì²´ ì‹¤í–‰ íë¦„
if __name__ == "__main__":
    all_keys = list_image_keys(BUCKET_NAME, PREFIX)
    sampled_keys = sample_image_keys(all_keys, SAMPLE_COUNT)
    print(f"ğŸ” S3ì—ì„œ {len(all_keys)}ê°œ ì¤‘ {len(sampled_keys)}ê°œ ìƒ˜í”Œë§")

    upload_id = create_upload_entry(PREFIX)
    insert_images_to_db(sampled_keys, upload_id)


# In[4]:


import os
import io
import boto3
import psycopg2
import numpy as np
import pandas as pd
from PIL import Image
from tqdm import tqdm
from datetime import datetime
import tensorflow as tf
import autokeras as ak
import mlflow
import mlflow.keras
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, roc_auc_score

# -------------------- ì„¤ì • --------------------
os.environ["CUDA_VISIBLE_DEVICES"] = "-1"  # GPU ì‚¬ìš© ì•ˆ í•¨

DB_INFO = {
    'host': 'localhost',
    'port': 5432,
    'dbname': 'postgres',
    'user': 'postgres',
    'password': '0523'
}

AWS_ACCESS_KEY = 'AKIA2UMF2NRTTGTVHHUD'
AWS_SECRET_KEY = 'Z/90YNZ1tiH2e8QEeXObw+incc99QIDWsrfN2bTb'
BUCKET_NAME = 'smart-factory-datalake'

image_size = (300, 300)
total_sample = 15  # í•™ìŠµì— ì‚¬ìš©í•  ì „ì²´ ìƒ˜í”Œ ìˆ˜
sample_threshold = 0.000000001  # ì¦ë¶„ ê¸°ì¤€ ë¹„ìœ¨
s3 = boto3.client("s3", aws_access_key_id=AWS_ACCESS_KEY, aws_secret_access_key=AWS_SECRET_KEY)

# -------------------- ì¦ë¶„ íŒë‹¨ --------------------
def is_incremental_significant(upload_id, threshold=sample_threshold):
    with psycopg2.connect(**DB_INFO) as conn:
        with conn.cursor() as cur:
            cur.execute("SELECT COUNT(*) FROM input_image")
            total = cur.fetchone()[0]
            cur.execute("SELECT COUNT(*) FROM input_image WHERE upload_id = %s", (upload_id,))
            new = cur.fetchone()[0]
    ratio = new / total if total > 0 else 1.0
    print(f"ğŸ” ê¸°ì¡´: {total - new}, ì‹ ê·œ: {new}, ë¹„ìœ¨: {ratio:.2%}")
    return ratio >= threshold

# -------------------- ìµœì‹  ëª¨ë¸ ì¡°íšŒ --------------------
def get_latest_model_accuracy_and_id():
    with psycopg2.connect(**DB_INFO) as conn:
        with conn.cursor() as cur:
            cur.execute("""
                SELECT id, accuracy FROM model_registry
                WHERE data_type = 'image' AND is_active = TRUE
                ORDER BY created_at DESC LIMIT 50
            """)
            row = cur.fetchone()
            return (row[0], row[1]) if row else (None, 0.0)

# -------------------- ì´ë¯¸ì§€ ë³‘í•© ìƒ˜í”Œë§ --------------------
def fetch_merged_sample_images(upload_id, total_sample=15):
    with psycopg2.connect(**DB_INFO) as conn:
        # ì‹ ê·œ ë°ì´í„° ì „ì²´
        df_new = pd.read_sql("""
            SELECT image_path, is_defect
            FROM input_image
            WHERE upload_id = %s
        """, conn, params=(upload_id,))

        remain = max(0, total_sample - len(df_new))
        if remain > 0:
            df_old = pd.read_sql(f"""
                SELECT image_path, is_defect
                FROM input_image
                WHERE upload_id != %s
                ORDER BY RANDOM()
                LIMIT {remain}
            """, conn, params=(upload_id,))
        else:
            df_old = pd.DataFrame(columns=['image_path', 'is_defect'])

    df = pd.concat([df_new, df_old]).reset_index(drop=True)

    x, y = [], []
    for _, row in tqdm(df.iterrows(), total=len(df), desc="ğŸ“¦ ì´ë¯¸ì§€ ë¡œë”©"):
        try:
            obj = s3.get_object(Bucket=BUCKET_NAME, Key=row["image_path"])
            img = Image.open(io.BytesIO(obj["Body"].read())).convert("RGB").resize(image_size)
            x.append(np.array(img, dtype=np.float32) / 255.0)
            y.append(int(row["is_defect"]))
        except Exception as e:
            print(f"âš ï¸ {row['image_path']} ë¡œë”© ì‹¤íŒ¨: {e}")

    return np.array(x, dtype=np.float32), np.array(y, dtype=np.int32)

# -------------------- í•™ìŠµ ë° ë“±ë¡ --------------------
def train_and_register_model(upload_id, total_sample=15, threshold=sample_threshold):
    if not is_incremental_significant(upload_id, threshold):
        print("ğŸ“Œ ì¦ë¶„ ë¶€ì¡±. í•™ìŠµ ìƒëµ.")
        return

    try:
        x, y = fetch_merged_sample_images(upload_id, total_sample)
    except Exception as e:
        print(f"âŒ ì´ë¯¸ì§€ ë¡œë”© ì‹¤íŒ¨: {e}")
        return

    if len(np.unique(y)) < 2:
        print("ğŸ“Œ í´ë˜ìŠ¤ ë¶ˆê· í˜•ìœ¼ë¡œ í•™ìŠµ ë¶ˆê°€")
        return

    x_train, x_test, y_train, y_test = train_test_split(
        x, y, test_size=0.2, stratify=y, random_state=42
    )

    clf = ak.ImageClassifier(overwrite=True, max_trials=1)
    clf = ak.ImageClassifier(overwrite=True, max_trials=1)
    clf.fit(
        x_train, y_train,
        epochs=5,
        validation_data=(x_test, y_test)  # âœ… ì´ ë¶€ë¶„ì„ ëª…ì‹œì ìœ¼ë¡œ ì§€ì •
    )

    y_pred = np.argmax(clf.predict(x_test), axis=1)
    new_accuracy = np.mean(y_pred == y_test)
    f1 = f1_score(y_test, y_pred)
    auc = roc_auc_score(y_test, y_pred)

    old_model_id, old_accuracy = get_latest_model_accuracy_and_id()
    print(f"ğŸ“ˆ ì´ì „ acc={old_accuracy:.4f}, ìƒˆ acc={new_accuracy:.4f} | f1={f1:.4f}, auc={auc:.4f}")

    was_accepted = False
    if new_accuracy > old_accuracy:
        decision = input("ğŸ“Œ ìƒˆ ëª¨ë¸ì„ ìš´ì˜ì— ë°˜ì˜í• ê¹Œìš”? (y/n): ").strip().lower()
        was_accepted = decision == 'y'

    model = clf.export_model()
    mlflow.set_experiment("SmartFactory_Image_Models")
    mlflow.set_tracking_uri("http://localhost:5000")

    with mlflow.start_run() as run:
        mlflow.keras.log_model(model, "model")
        mlflow.log_metrics({
            "accuracy": new_accuracy,
            "f1_score": f1,
            "auc_score": auc
        })

        version = datetime.now().strftime("v%Y%m%d_%H%M%S")
        mlflow_run_id = run.info.run_id
        model_name = run.info.run_name

        with psycopg2.connect(**DB_INFO) as conn:
            with conn.cursor() as cur:
                cur.execute("""
                    INSERT INTO model_registry (
                        model_name, version, mlflow_run_id,
                        trained_on_upload_id, data_type,
                        accuracy, f1_score, auc_score,
                        is_active, created_at
                    )
                    VALUES (%s, %s, %s, %s, 'image', %s, %s, %s, FALSE, NOW())
                """, (
                    'autokeras_image_model',
                    version,
                    mlflow_run_id,
                    upload_id,
                    new_accuracy,
                    f1,
                    auc
                ))

                cur.execute("SELECT id FROM model_registry WHERE mlflow_run_id = %s", (mlflow_run_id,))
                new_model_id = cur.fetchone()[0]

                if was_accepted:
                    cur.execute("UPDATE model_registry SET is_active = FALSE WHERE data_type = 'image' AND is_active = TRUE")
                    cur.execute("UPDATE model_registry SET is_active = TRUE WHERE id = %s", (new_model_id,))

                cur.execute("""
                    INSERT INTO model_selection_log (
                        data_type, old_model_id, new_model_id,
                        accuracy_diff, selected_by_user_id,
                        was_accepted, reason, selected_at
                    )
                    VALUES (%s, %s, %s, %s, NULL, %s, %s, NOW())
                """, (
                    'image',
                    old_model_id,
                    new_model_id,
                    round(new_accuracy - old_accuracy, 6),
                    was_accepted,
                    "User approved via prompt" if was_accepted else "User rejected via prompt"
                ))

            conn.commit()

    print("âœ… ëª¨ë¸ í•™ìŠµ ë° ê¸°ë¡ ì™„ë£Œ" if was_accepted else "ğŸ“Œ í•™ìŠµ ì™„ë£Œ (ìš´ì˜ ë¯¸ë°˜ì˜)")

# -------------------- ì‹¤í–‰ --------------------
if __name__ == "__main__":
    train_and_register_model(upload_id=2)



# In[ ]:




