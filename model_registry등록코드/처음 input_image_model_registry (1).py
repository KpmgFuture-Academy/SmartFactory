#!/usr/bin/env python
# coding: utf-8

# !pip install protobuf==3.20.*

# In[1]:


import tensorflow as tf
import autokeras as ak

print("‚úÖ TensorFlow:", tf.__version__)
print("‚úÖ AutoKeras:", ak.__version__)
print("‚úÖ GPU:", tf.config.list_physical_devices('GPU'))


# In[2]:


# ÏÉòÌîå Îç∞Ïù¥ÌÑ∞Î°ú 
import os
import io
import boto3
import psycopg2
import numpy as np
import pandas as pd
from PIL import Image
from datetime import datetime
from tqdm import tqdm
import tensorflow as tf
import autokeras as ak
import mlflow
import mlflow.keras
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, roc_auc_score

# ------------------------- ÏÑ§Ï†ï -------------------------
image_size = (300, 300)
sample_size = 3000

AWS_ACCESS_KEY = 'AKIA2UMF2NRTTGTVHHUD'
AWS_SECRET_KEY = 'Z/90YNZ1tiH2e8QEeXObw+incc99QIDWsrfN2bTb'
BUCKET_NAME = 'smart-factory-datalake'

DB_INFO = {
    'host': 'localhost',
    'port': 5432,
    'dbname': 'postgres',
    'user': 'postgres',
    'password': '0523'
}

s3 = boto3.client("s3", aws_access_key_id=AWS_ACCESS_KEY, aws_secret_access_key=AWS_SECRET_KEY)

def fetch_sampled_images(upload_id, sample_size):
    print(f"\nüü¢ DBÏóêÏÑú upload_id={upload_id} Îç∞Ïù¥ÌÑ∞ Î°úÎî© Ï§ë")
    conn = psycopg2.connect(**DB_INFO)
    df = pd.read_sql("""
        SELECT id AS image_id, image_path, is_defect
        FROM input_image
        WHERE upload_id = %s
    """, conn, params=(upload_id,))
    present_classes = set(df["is_defect"].unique())
    if len(present_classes) < 2:
        missing_class = 1 if 0 in present_classes else 0
        print(f"‚ö†Ô∏è ÌÅ¥ÎûòÏä§ {missing_class} Î≥¥ÏôÑÏùÑ ÏúÑÌï¥ DBÏóêÏÑú Ï∂îÍ∞Ä Ï°∞Ìöå")
        df_add = pd.read_sql(f"""
            SELECT id AS image_id, image_path, is_defect
            FROM input_image
            WHERE is_defect = {missing_class} AND upload_id != %s
            ORDER BY RANDOM()
            LIMIT 10
        """, conn, params=(upload_id,))
        df = pd.concat([df, df_add], ignore_index=True)
    conn.close()
    if df.empty:
        raise ValueError(f"‚ùå upload_id {upload_id}Ïóê Ìï¥ÎãπÌïòÎäî Ïù¥ÎØ∏ÏßÄÍ∞Ä ÏóÜÏäµÎãàÎã§.")
    df_sampled = df.sample(n=min(sample_size, len(df)), random_state=42).reset_index(drop=True)
    print(f"‚úÖ ÏÉòÌîåÎßÅ ÏôÑÎ£å: {len(df_sampled)}Í∞ú (Îëê ÌÅ¥ÎûòÏä§ Ìè¨Ìï®Îê®)")
    x, y = [], []
    for _, row in tqdm(df_sampled.iterrows(), total=len(df_sampled), desc="üì¶ Ïù¥ÎØ∏ÏßÄ Î°úÎî©"):
        try:
            key = row["image_path"]
            label = int(row["is_defect"])
            obj = s3.get_object(Bucket=BUCKET_NAME, Key=key)
            img = Image.open(io.BytesIO(obj["Body"].read())).convert("RGB").resize(image_size)
            x.append(np.array(img) / 255.0)
            y.append(label)
        except Exception as e:
            print(f"‚ö†Ô∏è {key} ‚Üí {e}")
    return np.array(x, dtype=np.float32), np.array(y, dtype=np.int32)

def train_and_log_model(upload_id):
    x, y = fetch_sampled_images(upload_id, sample_size)
    if len(np.unique(y)) < 2:
        raise ValueError("‚ùå ÌÅ¥ÎûòÏä§Í∞Ä ÌïòÎÇòÎßå Ï°¥Ïû¨Ìï©ÎãàÎã§. ÏµúÏÜå Îëê ÌÅ¥ÎûòÏä§Í∞Ä ÌïÑÏöîÌï©ÎãàÎã§.")
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, stratify=y, random_state=42)
    clf = ak.ImageClassifier(overwrite=True, max_trials=1)
    clf.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))
    from sklearn.metrics import accuracy_score, f1_score, roc_auc_score
    try:
        y_pred_probs = clf.predict_proba(x_test)
    except AttributeError:
        y_pred_probs = clf.predict(x_test)
        print("‚ö†Ô∏è predict_probaÍ∞Ä ÏóÜÏùå. predict ÏÇ¨Ïö©. ÌôïÎ•† Ï∂úÎ†•ÏúºÎ°ú Î≥ÄÌôò:")
        y_pred_probs = y_pred_probs.flatten()  # (n_samples, 1) ‚Üí (n_samples,)
    y_pred = np.round(y_pred_probs).astype(int).flatten()
    try:
        accuracy = accuracy_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred, average='binary', zero_division=0)
        n_classes = len(np.unique(y_test))
        if n_classes < 2:
            auc = 0.0
        else:
            auc = roc_auc_score(y_test, y_pred_probs)
        print(f"Accuracy: {accuracy:.4f}")
        print(f"F1-Score: {f1:.4f}")
        print(f"AUC: {auc:.4f}")
    except Exception as e:
        print(f"‚ö†Ô∏è ÏßÄÌëú Í≥ÑÏÇ∞ Ïã§Ìå®: {e}")
        print(f"ÎîîÎ≤ÑÍπÖ Ï†ïÎ≥¥ - y_test: {np.unique(y_test, return_counts=True)}")
        print(f"ÎîîÎ≤ÑÍπÖ Ï†ïÎ≥¥ - y_pred_probs shape: {y_pred_probs.shape}")
        accuracy, f1, auc = 0.0, 0.0, 0.0
    model = clf.export_model()
    mlflow.set_tracking_uri("http://localhost:5000")
    mlflow.set_experiment("SmartFactory_Image_Models")
    with mlflow.start_run() as run:
        mlflow.keras.log_model(model, "model")
        mlflow.log_metrics({"accuracy": accuracy, "f1_score": f1, "auc_score": auc})
        mlflow_run_id = run.info.run_id
        model_name = run.info.run_name
        version = datetime.now().strftime("v%Y%m%d_%H%M%S")
        conn = psycopg2.connect(**DB_INFO)
        cur = conn.cursor()
        cur.execute('''
            INSERT INTO model_registry (
                model_name, version, mlflow_run_id,
                trained_on_upload_id, data_type,
                accuracy, f1_score, auc_score,
                is_active, created_at
            ) VALUES (%s, %s, %s, %s, 'image', %s, %s, %s, TRUE, NOW())
        ''', (model_name, version, mlflow_run_id, upload_id, accuracy, f1, auc))
        conn.commit()
        cur.close()
        conn.close()
        print(f"‚úÖ Î™®Îç∏ Îì±Î°ù ÏôÑÎ£å: {model_name}, version={version}, Run ID={mlflow_run_id}")

if __name__ == "__main__":
    upload_id = 1
    train_and_log_model(upload_id)


# In[ ]:


import os
os.environ["CUDA_VISIBLE_DEVICES"] = "-1"  # GPU ÏÇ¨Ïö© Ïïà Ìï®
# Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞Î°ú
import os
import io
import boto3
import psycopg2
import numpy as np
import pandas as pd
from PIL import Image
from datetime import datetime
from tqdm import tqdm
import tensorflow as tf
import autokeras as ak
import mlflow
import mlflow.keras
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, roc_auc_score
# ------------------------- ÏÑ§Ï†ï -------------------------

# TensorFlow GPU ÎπÑÌôúÏÑ±Ìôî Î™ÖÏãú
physical_devices = tf.config.list_physical_devices('GPU')
if physical_devices:
    tf.config.set_visible_devices([], 'GPU')
    print("‚úÖ GPU ÎπÑÌôúÏÑ±Ìôî ÏôÑÎ£å")
else:
    print("‚úÖ GPU ÏóÜÏùå, CPUÎ°ú ÏßÑÌñâ")

DB_INFO = {
    'host': 'localhost',
    'port': 5432,
    'dbname': 'postgres',
    'user': 'postgres',
    'password': '0523'
}

AWS_ACCESS_KEY = 'AKIA2UMF2NRTTGTVHHUD'
AWS_SECRET_KEY = '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'
BUCKET_NAME = 'smart-factory-datalake'

image_size = (300, 300)

s3 = boto3.client("s3", aws_access_key_id=AWS_ACCESS_KEY, aws_secret_access_key=AWS_SECRET_KEY)

def fetch_sampled_images(upload_id):
    print(f"\nüü¢ DBÏóêÏÑú upload_id={upload_id} Îç∞Ïù¥ÌÑ∞ Î°úÎî© Ï§ë")
    conn = psycopg2.connect(**DB_INFO)
    df = pd.read_sql("""
        SELECT id AS image_id, image_path, is_defect
        FROM input_image
        WHERE upload_id = %s
    """, conn, params=(upload_id,))
    present_classes = set(df["is_defect"].unique())
    if len(present_classes) < 2:
        missing_class = 1 if 0 in present_classes else 0
        print(f"‚ö†Ô∏è ÌÅ¥ÎûòÏä§ {missing_class} Î≥¥ÏôÑÏùÑ ÏúÑÌï¥ DBÏóêÏÑú Ï∂îÍ∞Ä Ï°∞Ìöå")
        df_add = pd.read_sql(f"""
            SELECT id AS image_id, image_path, is_defect
            FROM input_image
            WHERE is_defect = {missing_class} AND upload_id != %s
            ORDER BY RANDOM()
            LIMIT 10
        """, conn, params=(upload_id,))
        df = pd.concat([df, df_add], ignore_index=True)
    conn.close()
    if df.empty:
        raise ValueError(f"‚ùå upload_id {upload_id}Ïóê Ìï¥ÎãπÌïòÎäî Ïù¥ÎØ∏ÏßÄÍ∞Ä ÏóÜÏäµÎãàÎã§.")
    df_full = df.reset_index(drop=True)  # ÏÉòÌîåÎßÅ Ï†úÍ±∞, Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞ ÏÇ¨Ïö©
    print(f"‚úÖ Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞ Î°úÎìú ÏôÑÎ£å: {len(df_full)}Í∞ú (Îëê ÌÅ¥ÎûòÏä§ Ìè¨Ìï®Îê®)")
    x, y = [], []
    for _, row in tqdm(df_full.iterrows(), total=len(df_full), desc="üì¶ Ïù¥ÎØ∏ÏßÄ Î°úÎî©"):
        try:
            key = row["image_path"]
            label = int(row["is_defect"])
            obj = s3.get_object(Bucket=BUCKET_NAME, Key=key)
            img = Image.open(io.BytesIO(obj["Body"].read())).convert("RGB").resize(image_size)
            x.append(np.array(img) / 255.0)
            y.append(label)
        except Exception as e:
            print(f"‚ö†Ô∏è {key} ‚Üí {e}")
    return np.array(x, dtype=np.float32), np.array(y, dtype=np.int32)

def train_and_log_model(upload_id):
    x, y = fetch_sampled_images(upload_id)
    if len(np.unique(y)) < 2:
        raise ValueError("‚ùå ÌÅ¥ÎûòÏä§Í∞Ä ÌïòÎÇòÎßå Ï°¥Ïû¨Ìï©ÎãàÎã§. ÏµúÏÜå Îëê ÌÅ¥ÎûòÏä§Í∞Ä ÌïÑÏöîÌï©ÎãàÎã§.")
    
    # Îç∞Ïù¥ÌÑ∞ ÌÅ¨Í∏∞ ÌôïÏù∏
    print(f"üìè Îç∞Ïù¥ÌÑ∞ ÌÅ¨Í∏∞ - x: {x.shape}, y: {y.shape}")
    
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, stratify=y, random_state=42)
    
    # ÌïôÏäµ Îç∞Ïù¥ÌÑ∞ ÌÅ¨Í∏∞ ÌôïÏù∏
    print(f"üìè ÌïôÏäµ Îç∞Ïù¥ÌÑ∞ ÌÅ¨Í∏∞ - x_train: {x_train.shape}, y_train: {y_train.shape}")
    
    clf = ak.ImageClassifier(overwrite=True, max_trials=1)
    try:
        clf.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))
    except Exception as e:
        print(f"‚ùå ÌïôÏäµ Ïã§Ìå®: {e}")
        print(f"ÎîîÎ≤ÑÍπÖ Ï†ïÎ≥¥ - x_train shape: {x_train.shape}, y_train shape: {y_train.shape}")
        raise
    
    from sklearn.metrics import accuracy_score, f1_score, roc_auc_score
    try:
        y_pred_probs = clf.predict_proba(x_test)
    except AttributeError:
        y_pred_probs = clf.predict(x_test)
        print("‚ö†Ô∏è predict_probaÍ∞Ä ÏóÜÏùå. predict ÏÇ¨Ïö©. ÌôïÎ•† Ï∂úÎ†•ÏúºÎ°ú Î≥ÄÌôò:")
        y_pred_probs = y_pred_probs.flatten()  # (n_samples, 1) ‚Üí (n_samples,)
    y_pred = np.round(y_pred_probs).astype(int).flatten()
    try:
        accuracy = accuracy_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred, average='binary', zero_division=0)
        n_classes = len(np.unique(y_test))
        if n_classes < 2:
            auc = 0.0
        else:
            auc = roc_auc_score(y_test, y_pred_probs)
        print(f"Accuracy: {accuracy:.4f}")
        print(f"F1-Score: {f1:.4f}")
        print(f"AUC: {auc:.4f}")
    except Exception as e:
        print(f"‚ö†Ô∏è ÏßÄÌëú Í≥ÑÏÇ∞ Ïã§Ìå®: {e}")
        print(f"ÎîîÎ≤ÑÍπÖ Ï†ïÎ≥¥ - y_test: {np.unique(y_test, return_counts=True)}")
        print(f"ÎîîÎ≤ÑÍπÖ Ï†ïÎ≥¥ - y_pred_probs shape: {y_pred_probs.shape}")
        accuracy, f1, auc = 0.0, 0.0, 0.0
    model = clf.export_model()
    mlflow.set_tracking_uri("http://localhost:5000")
    mlflow.set_experiment("SmartFactory_Image_Models")
    with mlflow.start_run() as run:
        mlflow.keras.log_model(model, "model")
        mlflow.log_metrics({"accuracy": accuracy, "f1_score": f1, "auc_score": auc})
        mlflow_run_id = run.info.run_id
        model_name = run.info.run_name
        version = datetime.now().strftime("v%Y%m%d_%H%M%S")
        conn = psycopg2.connect(**DB_INFO)
        cur = conn.cursor()
        cur.execute('''
            INSERT INTO model_registry (
                model_name, version, mlflow_run_id,
                trained_on_upload_id, data_type,
                accuracy, f1_score, auc_score,
                is_active, created_at
            ) VALUES (%s, %s, %s, %s, 'image', %s, %s, %s, TRUE, NOW())
        ''', (model_name, version, mlflow_run_id, upload_id, accuracy, f1, auc))
        conn.commit()
        cur.close()
        conn.close()
        print(f"‚úÖ Î™®Îç∏ Îì±Î°ù ÏôÑÎ£å: {model_name}, version={version}, Run ID={mlflow_run_id}")

if __name__ == "__main__":
    upload_id = 1
    train_and_log_model(upload_id)


# In[ ]:


# 1. Test dataÎ•º DB Ï†ÅÏû¨ ÌõÑ 
# 2. Í∏∞Ï°¥ Îç∞Ïù¥ÌÑ∞ Ï†ÅÏû¨ Ïàò(input_imageÏùò Ïàò count)ÏôÄ ÏÉàÎ°ú Îì§Ïñ¥Ïò® Test data Ïàò ÎπÑÍµê 
# 


# In[ ]:


# 3. Î™®Îç∏ Ïû¨ÌïôÏäµ(autokeras)


# In[ ]:


# 4. Í∏∞Ï°¥ Î™®Îç∏ vs ÏÉà Î™®Îç∏


# In[ ]:


# 5. ÏÇ¨Ïö©Ïûê ÏÑ†ÌÉù


# In[ ]:


# 6. ÏÉà Î™®Îç∏ registry Îì±Î°ù


# In[ ]:


# 7. Ïö¥ÏòÅ Î∞∞Ìè¨ ÌååÏù¥ÌîÑÎùºÏù∏ Ïó∞Îèô


# In[3]:


# test ÏúÑÌïú S3 test Îç∞Ïù¥ÌÑ∞ Ï∂îÏ∂ú ÌõÑ DB Ï†ÅÏû¨ 
import os
import boto3
import psycopg2
import random
from datetime import datetime
from PIL import Image
import io

# ÌôòÍ≤Ω ÏÑ§Ï†ï
AWS_ACCESS_KEY = 'AKIA2UMF2NRTTGTVHHUD'
AWS_SECRET_KEY = 'Z/90YNZ1tiH2e8QEeXObw+incc99QIDWsrfN2bTb'
BUCKET_NAME = 'smart-factory-datalake'
PREFIX = 'test/'  # ‚úÖ test Îç∞Ïù¥ÌÑ∞ ÏúÑÏπò
SAMPLE_COUNT = 10          # ‚úÖ Í≥†Ï†ïÎêú Í∞úÏàò ÏÉòÌîåÎßÅ

# ÌÖåÏä§Ìä∏Ïö© Í≥†Ï†ï ID
COMPANY_ID = '11111111-1111-1111-1111-111111111111'
UPLOADER_ID = '22222222-2222-2222-2222-222222222222'

DB_INFO = {
    'host': 'localhost',
    'port': 5432,
    'dbname': 'postgres',
    'user': 'postgres',
    'password': '0523'
}

s3 = boto3.client("s3", aws_access_key_id=AWS_ACCESS_KEY, aws_secret_access_key=AWS_SECRET_KEY)

# Step 1. S3 ÌÇ§ Î¶¨Ïä§Ìä∏ Î∂àÎü¨Ïò§Í∏∞
def list_image_keys(bucket, prefix):
    response = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)
    return [obj['Key'] for obj in response.get('Contents', []) if obj['Key'].lower().endswith(('.jpg', '.jpeg', '.png'))]

# Step 2. 10Í∞úÎßå ÏÉòÌîåÎßÅ
def sample_image_keys(all_keys, count):
    return random.sample(all_keys, min(count, len(all_keys)))

# Step 3. uploads ÌÖåÏù¥Î∏îÏóê ÏóÖÎ°úÎìú Î©îÌÉÄ ÏÉùÏÑ±
def create_upload_entry(file_key_prefix):
    conn = psycopg2.connect(**DB_INFO)
    cur = conn.cursor()
    cur.execute("""
        INSERT INTO uploads (company_id, uploader_id, file_key, data_type, has_target, label_type, task_type, created_at)
        VALUES (%s, %s, %s, 'image', FALSE, 'binary', NULL, NOW())
        RETURNING upload_id
    """, (COMPANY_ID, UPLOADER_ID, file_key_prefix))
    upload_id = cur.fetchone()[0]
    conn.commit()
    cur.close()
    conn.close()
    print(f"üì• uploads ÌÖåÏù¥Î∏îÏóê Îì±Î°ù ÏôÑÎ£å ‚Üí upload_id: {upload_id}")
    return upload_id

# Step 4. S3 Ïù¥ÎØ∏ÏßÄ Î©îÌÉÄ Ï∂îÏ∂ú
def get_image_metadata(key):
    obj = s3.get_object(Bucket=BUCKET_NAME, Key=key)
    img = Image.open(io.BytesIO(obj['Body'].read()))
    width, height = img.size
    img_format = img.format.lower()
    return width, height, img_format

# Step 5. input_imageÏóê ÏÇΩÏûÖ
def insert_images_to_db(image_keys, upload_id):
    conn = psycopg2.connect(**DB_INFO)
    cur = conn.cursor()

    for key in image_keys:
        try:
            width, height, img_format = get_image_metadata(key)
            is_defect = random.choice([True, False])  # ÏûÑÏùòÎ°ú ÎùºÎ≤® Î∂ÄÏó¨
            cur.execute("""
                INSERT INTO input_image (upload_id, image_path, width, height, format, is_defect, created_at)
                VALUES (%s, %s, %s, %s, %s, %s, NOW())
            """, (upload_id, key, width, height, img_format, is_defect))
            print(f"‚úÖ Îì±Î°ù ÏôÑÎ£å: {key}")
        except Exception as e:
            print(f"‚ö†Ô∏è Îì±Î°ù Ïã§Ìå®: {key} ‚Üí {e}")

    conn.commit()
    cur.close()
    conn.close()
    print(f"üì¶ Ï¥ù {len(image_keys)}Í∞ú Ïù¥ÎØ∏ÏßÄ Îì±Î°ù ÏôÑÎ£å (upload_id: {upload_id})")

# Ï†ÑÏ≤¥ Ïã§Ìñâ ÌùêÎ¶Ñ
if __name__ == "__main__":
    all_keys = list_image_keys(BUCKET_NAME, PREFIX)
    sampled_keys = sample_image_keys(all_keys, SAMPLE_COUNT)
    print(f"üîç S3ÏóêÏÑú {len(all_keys)}Í∞ú Ï§ë {len(sampled_keys)}Í∞ú ÏÉòÌîåÎßÅ")

    upload_id = create_upload_entry(PREFIX)
    insert_images_to_db(sampled_keys, upload_id)


# In[4]:


import os
import io
import boto3
import psycopg2
import numpy as np
import pandas as pd
from PIL import Image
from tqdm import tqdm
from datetime import datetime
import tensorflow as tf
import autokeras as ak
import mlflow
import mlflow.keras
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, roc_auc_score

# -------------------- ÏÑ§Ï†ï --------------------
os.environ["CUDA_VISIBLE_DEVICES"] = "-1"  # GPU ÏÇ¨Ïö© Ïïà Ìï®

DB_INFO = {
    'host': 'localhost',
    'port': 5432,
    'dbname': 'postgres',
    'user': 'postgres',
    'password': '0523'
}

AWS_ACCESS_KEY = 'AKIA2UMF2NRTTGTVHHUD'
AWS_SECRET_KEY = 'Z/90YNZ1tiH2e8QEeXObw+incc99QIDWsrfN2bTb'
BUCKET_NAME = 'smart-factory-datalake'

image_size = (300, 300)
total_sample = 15  # ÌïôÏäµÏóê ÏÇ¨Ïö©Ìï† Ï†ÑÏ≤¥ ÏÉòÌîå Ïàò
sample_threshold = 0.000000001  # Ï¶ùÎ∂Ñ Í∏∞Ï§Ä ÎπÑÏú®
s3 = boto3.client("s3", aws_access_key_id=AWS_ACCESS_KEY, aws_secret_access_key=AWS_SECRET_KEY)

# -------------------- Ï¶ùÎ∂Ñ ÌåêÎã® --------------------
def is_incremental_significant(upload_id, threshold=sample_threshold):
    with psycopg2.connect(**DB_INFO) as conn:
        with conn.cursor() as cur:
            cur.execute("SELECT COUNT(*) FROM input_image")
            total = cur.fetchone()[0]
            cur.execute("SELECT COUNT(*) FROM input_image WHERE upload_id = %s", (upload_id,))
            new = cur.fetchone()[0]
    ratio = new / total if total > 0 else 1.0
    print(f"üîç Í∏∞Ï°¥: {total - new}, Ïã†Í∑ú: {new}, ÎπÑÏú®: {ratio:.2%}")
    return ratio >= threshold

# -------------------- ÏµúÏã† Î™®Îç∏ Ï°∞Ìöå --------------------
def get_latest_model_accuracy_and_id():
    with psycopg2.connect(**DB_INFO) as conn:
        with conn.cursor() as cur:
            cur.execute("""
                SELECT id, accuracy FROM model_registry
                WHERE data_type = 'image' AND is_active = TRUE
                ORDER BY created_at DESC LIMIT 50
            """)
            row = cur.fetchone()
            return (row[0], row[1]) if row else (None, 0.0)

# -------------------- Ïù¥ÎØ∏ÏßÄ Î≥ëÌï© ÏÉòÌîåÎßÅ --------------------
def fetch_merged_sample_images(upload_id, total_sample=15):
    with psycopg2.connect(**DB_INFO) as conn:
        # Ïã†Í∑ú Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤¥
        df_new = pd.read_sql("""
            SELECT image_path, is_defect
            FROM input_image
            WHERE upload_id = %s
        """, conn, params=(upload_id,))

        remain = max(0, total_sample - len(df_new))
        if remain > 0:
            df_old = pd.read_sql(f"""
                SELECT image_path, is_defect
                FROM input_image
                WHERE upload_id != %s
                ORDER BY RANDOM()
                LIMIT {remain}
            """, conn, params=(upload_id,))
        else:
            df_old = pd.DataFrame(columns=['image_path', 'is_defect'])

    df = pd.concat([df_new, df_old]).reset_index(drop=True)

    x, y = [], []
    for _, row in tqdm(df.iterrows(), total=len(df), desc="üì¶ Ïù¥ÎØ∏ÏßÄ Î°úÎî©"):
        try:
            obj = s3.get_object(Bucket=BUCKET_NAME, Key=row["image_path"])
            img = Image.open(io.BytesIO(obj["Body"].read())).convert("RGB").resize(image_size)
            x.append(np.array(img, dtype=np.float32) / 255.0)
            y.append(int(row["is_defect"]))
        except Exception as e:
            print(f"‚ö†Ô∏è {row['image_path']} Î°úÎî© Ïã§Ìå®: {e}")

    return np.array(x, dtype=np.float32), np.array(y, dtype=np.int32)

# -------------------- ÌïôÏäµ Î∞è Îì±Î°ù --------------------
def train_and_register_model(upload_id, total_sample=15, threshold=sample_threshold):
    if not is_incremental_significant(upload_id, threshold):
        print("üìå Ï¶ùÎ∂Ñ Î∂ÄÏ°±. ÌïôÏäµ ÏÉùÎûµ.")
        return

    try:
        x, y = fetch_merged_sample_images(upload_id, total_sample)
    except Exception as e:
        print(f"‚ùå Ïù¥ÎØ∏ÏßÄ Î°úÎî© Ïã§Ìå®: {e}")
        return

    if len(np.unique(y)) < 2:
        print("üìå ÌÅ¥ÎûòÏä§ Î∂àÍ∑†ÌòïÏúºÎ°ú ÌïôÏäµ Î∂àÍ∞Ä")
        return

    x_train, x_test, y_train, y_test = train_test_split(
        x, y, test_size=0.2, stratify=y, random_state=42
    )

    clf = ak.ImageClassifier(overwrite=True, max_trials=1)
    clf = ak.ImageClassifier(overwrite=True, max_trials=1)
    clf.fit(
        x_train, y_train,
        epochs=5,
        validation_data=(x_test, y_test)  # ‚úÖ Ïù¥ Î∂ÄÎ∂ÑÏùÑ Î™ÖÏãúÏ†ÅÏúºÎ°ú ÏßÄÏ†ï
    )

    y_pred = np.argmax(clf.predict(x_test), axis=1)
    new_accuracy = np.mean(y_pred == y_test)
    f1 = f1_score(y_test, y_pred)
    auc = roc_auc_score(y_test, y_pred)

    old_model_id, old_accuracy = get_latest_model_accuracy_and_id()
    print(f"üìà Ïù¥Ï†Ñ acc={old_accuracy:.4f}, ÏÉà acc={new_accuracy:.4f} | f1={f1:.4f}, auc={auc:.4f}")

    was_accepted = False
    if new_accuracy > old_accuracy:
        decision = input("üìå ÏÉà Î™®Îç∏ÏùÑ Ïö¥ÏòÅÏóê Î∞òÏòÅÌï†ÍπåÏöî? (y/n): ").strip().lower()
        was_accepted = decision == 'y'

    model = clf.export_model()
    mlflow.set_experiment("SmartFactory_Image_Models")
    mlflow.set_tracking_uri("http://localhost:5000")

    with mlflow.start_run() as run:
        mlflow.keras.log_model(model, "model")
        mlflow.log_metrics({
            "accuracy": new_accuracy,
            "f1_score": f1,
            "auc_score": auc
        })

        version = datetime.now().strftime("v%Y%m%d_%H%M%S")
        mlflow_run_id = run.info.run_id
        model_name = run.info.run_name

        with psycopg2.connect(**DB_INFO) as conn:
            with conn.cursor() as cur:
                cur.execute("""
                    INSERT INTO model_registry (
                        model_name, version, mlflow_run_id,
                        trained_on_upload_id, data_type,
                        accuracy, f1_score, auc_score,
                        is_active, created_at
                    )
                    VALUES (%s, %s, %s, %s, 'image', %s, %s, %s, FALSE, NOW())
                """, (
                    'autokeras_image_model',
                    version,
                    mlflow_run_id,
                    upload_id,
                    new_accuracy,
                    f1,
                    auc
                ))

                cur.execute("SELECT id FROM model_registry WHERE mlflow_run_id = %s", (mlflow_run_id,))
                new_model_id = cur.fetchone()[0]

                if was_accepted:
                    cur.execute("UPDATE model_registry SET is_active = FALSE WHERE data_type = 'image' AND is_active = TRUE")
                    cur.execute("UPDATE model_registry SET is_active = TRUE WHERE id = %s", (new_model_id,))

                cur.execute("""
                    INSERT INTO model_selection_log (
                        data_type, old_model_id, new_model_id,
                        accuracy_diff, selected_by_user_id,
                        was_accepted, reason, selected_at
                    )
                    VALUES (%s, %s, %s, %s, NULL, %s, %s, NOW())
                """, (
                    'image',
                    old_model_id,
                    new_model_id,
                    round(new_accuracy - old_accuracy, 6),
                    was_accepted,
                    "User approved via prompt" if was_accepted else "User rejected via prompt"
                ))

            conn.commit()

    print("‚úÖ Î™®Îç∏ ÌïôÏäµ Î∞è Í∏∞Î°ù ÏôÑÎ£å" if was_accepted else "üìå ÌïôÏäµ ÏôÑÎ£å (Ïö¥ÏòÅ ÎØ∏Î∞òÏòÅ)")

# -------------------- Ïã§Ìñâ --------------------
if __name__ == "__main__":
    train_and_register_model(upload_id=2)



# In[ ]:




